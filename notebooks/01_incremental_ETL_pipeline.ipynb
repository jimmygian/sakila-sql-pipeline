{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b998da6",
   "metadata": {},
   "source": [
    "# Incremental ETL Pipeline\n",
    "\n",
    "This notebook orchestrates the flow of data from the transactional source database (sakila) to the analytical data warehouse (sakila_star).\n",
    "\n",
    "Key Features:\n",
    "- Incremental Loading: Uses a \"Watermark\" strategy to fetch only records created or modified since the last successful run.\n",
    "- Star Schema Transformation: Denormalizes data (e.g., joining address, city, and country into dimension tables).\n",
    "- Upsert Logic: Uses ON DUPLICATE KEY UPDATE to handle modifications to existing records.\n",
    "- Deletion Sync: Identifies and removes records in the warehouse that have been hard-deleted from the source. \\[WIP\\]\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7f4da2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Source: Transactional Database\n",
    "src_engine = create_engine(\"mysql+pymysql://app:app_password@db:3306/sakila\")\n",
    "\n",
    "# Target: Data Warehouse\n",
    "tgt_engine = create_engine(\"mysql+pymysql://app:app_password@db:3306/sakila_star\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f158ef-371e-4539-ade8-f76f67e6305a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Check schemas (Optional)\n",
    "\n",
    "Check if you can read from both databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cabd0a-7594-452d-9ecb-5b7bdbee863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SHOW FULL TABLES;\", src_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab46620-70e0-4778-8ffd-f3967c8e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SHOW FULL TABLES;\", tgt_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c6c87-7e2c-4109-96fe-09e6e2ac6cb9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Core ETL Functions:\n",
    "\n",
    "These functions are important for all of this notebook's operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82c805-2504-414a-8d80-d2b013ff3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watermark(pipeline_name, conn):\n",
    "    \"\"\"Retrieve the last successful timestamp from sakila_star.etl_state\"\"\"\n",
    "    \n",
    "    query = text(\"SELECT last_success_ts FROM etl_state WHERE pipeline_name = :p_name\")\n",
    "    \n",
    "    result = conn.execute(\n",
    "        query, \n",
    "        {\"p_name\": pipeline_name}\n",
    "    ).fetchone()\n",
    "\n",
    "    \n",
    "    # If never run, default to an old timestamp (start of epoch)\n",
    "    return result[0] if result else '1970-01-01 00:00:00'\n",
    "\n",
    "# Test\n",
    "with tgt_engine.connect() as conn:\n",
    "    watermark_value = get_watermark(\"fact_rental\", conn)\n",
    "    print(watermark_value)\n",
    "\n",
    "\n",
    "\n",
    "def update_watermark(pipeline_names, new_ts, conn):\n",
    "    \"\"\"\n",
    "    Update the watermark for a list of pipelines.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_names (list): A list of pipeline names (e.g. ['dim_customer'])\n",
    "        new_ts (datetime/str): The timestamp to set (e.g. '2025-01-01 12:00:00')\n",
    "        conn: The active database connection\n",
    "    \"\"\"\n",
    "    \n",
    "    query = text(\"\"\"\n",
    "        INSERT INTO etl_state (pipeline_name, last_success_ts) \n",
    "        VALUES (:p_name, :ts) \n",
    "        ON DUPLICATE KEY UPDATE last_success_ts = VALUES(last_success_ts)\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    for name in pipeline_names:\n",
    "        old_watermark = get_watermark(name, conn)\n",
    "        \n",
    "        conn.execute(query, {\"p_name\": name, \"ts\": new_ts})\n",
    "        print(f\"[{name}] Updated watermark from '{old_watermark}' to '{new_ts}' in 'etl_state' table\")\n",
    "    \n",
    "\n",
    "def test_select_query(query, engine, watermark_value='1970-01-01 00:00:00'):    \n",
    "    return (\n",
    "        pd.read_sql(\n",
    "            query, \n",
    "            tgt_engine, \n",
    "            params={\"watermark\": watermark_value}\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _initialise_etl_state(etl_state_list=None, ts='1970-01-01 00:00:00'):\n",
    "    print(\"\\n\\n >> INITIALISING ETL STATE ...\\n\")\n",
    "\n",
    "    if not etl_state_list:\n",
    "        etl_state_list = [\"fact_rental\", \"dim_film\", \"dim_customer\", \"dim_staff\", \"dim_actor\", \"bridge_actor\", \"dim_store\"]\n",
    "    \n",
    "    with tgt_engine.connect() as conn:\n",
    "        update_watermark(etl_state_list, ts, conn)\n",
    "        conn.commit() \n",
    "\n",
    "def _clear_table_data(table_names, engine, force=False):\n",
    "    \"\"\"\n",
    "    Deletes all rows from a specified tables.\n",
    "    \n",
    "    Args:\n",
    "        table_name (list): The tables to clear (e.g. ['dim_store'])\n",
    "        engine: The SQLAlchemy engine to use\n",
    "        force (bool): If True, disables Foreign Key checks to force deletion.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n >> CLEARING DATA FROM TABLES {table_names} ...\\n\")\n",
    "    for table_name in table_names:\n",
    "        with engine.connect() as conn:\n",
    "            try:\n",
    "                if force:\n",
    "                    conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 0\"))\n",
    "                \n",
    "                # Delete all rows\n",
    "                result = conn.execute(text(f\"DELETE FROM {table_name}\"))\n",
    "                \n",
    "                if force:\n",
    "                    conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 1\"))\n",
    "                \n",
    "                conn.commit()\n",
    "                print(f\"Success: Deleted {result.rowcount} rows from {table_name}.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error clearing {table_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_incremental_load(pipeline_name, extract_sql, load_sql, src_engine, tgt_engine):\n",
    "    \"\"\"\n",
    "    Generic function to run ETL for a single table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get Watermark (Target)\n",
    "    with tgt_engine.connect() as tgt_conn:\n",
    "        watermark = get_watermark(pipeline_name, tgt_conn)\n",
    "        \n",
    "    print(f\"[{pipeline_name}] Checking for updates since {watermark}...\")\n",
    "\n",
    "    # 2. Extract Data (Source)\n",
    "    # We use the watermark to only fetch new data\n",
    "    try:\n",
    "        df = pd.read_sql(extract_sql, src_engine, params={\"watermark\": watermark})\n",
    "    except Exception as e:\n",
    "        print(f\"[{pipeline_name}] Extraction Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[{pipeline_name}] No new records found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{pipeline_name}] Found {len(df)} rows. Loading...\")\n",
    "\n",
    "    # 3. Load Data (Target)\n",
    "    # Iterate and execute the specific Upsert SQL for this table\n",
    "    with tgt_engine.begin() as tgt_conn: # .begin() auto-commits on success\n",
    "        for index, row in df.iterrows():\n",
    "        \n",
    "            # Convert row to dict for parameter binding\n",
    "            tgt_conn.execute(load_sql, row.to_dict())\n",
    "        \n",
    "        # 4. Update Watermark (Target)\n",
    "        # Calculate new max timestamp from the dataframe\n",
    "        new_ts = df['src_last_update'].max()\n",
    "        update_watermark([pipeline_name], new_ts, tgt_conn)\n",
    "        \n",
    "    print(f\"[{pipeline_name}] Success! Watermark updated to {new_ts}\")\n",
    "\n",
    "def upsert_data(upsert_list, src_engine, tgt_engine):\n",
    "    # print(upsert_list)\n",
    "\n",
    "    print(\"\\n\\n >> UPDATING / INSERTING DATA ...\\n\")\n",
    "\n",
    "    for job in upsert_list:\n",
    "        run_incremental_load(\n",
    "            job[\"table_name\"], \n",
    "            job[\"extract_sql\"], \n",
    "            job[\"load_sql\"], \n",
    "            src_engine, \n",
    "            tgt_engine\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02fa12-19d9-4aee-b843-868f709f987e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Extract and Load SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a39fba-fbf3-473b-9e93-d82cfaf6e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= DIM_STORE ========= ## ===== GET SRC DATA STATEMENTS ===== #\n",
    "\n",
    "\n",
    "# Get 'dim_store' data from src\n",
    "dim_store_extract_sql = text(\"\"\"\n",
    "    SELECT \n",
    "        s.store_id, \n",
    "        a.address, \n",
    "        a.address2, \n",
    "        a.district, \n",
    "        c.city, \n",
    "        co.country, \n",
    "        a.postal_code, \n",
    "        a.phone, \n",
    "        s.last_update as src_last_update\n",
    "    FROM sakila.store s\n",
    "    JOIN sakila.address a ON s.address_id = a.address_id\n",
    "    JOIN sakila.city c ON a.city_id = c.city_id\n",
    "    JOIN sakila.country co ON c.country_id = co.country_id\n",
    "    WHERE s.last_update > :watermark\n",
    "\"\"\")\n",
    "\n",
    "dim_customer_extract_sql = text(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id, \n",
    "        c.store_id, \n",
    "        c.first_name, \n",
    "        c.last_name, \n",
    "        c.email, \n",
    "        c.active as activebool,  -- Map source 'active' to target 'activebool'\n",
    "        c.active,                -- Also keep 'active' for the second column\n",
    "        c.create_date, \n",
    "        a.address, \n",
    "        a.address2,              \n",
    "        a.district,              \n",
    "        ci.city, \n",
    "        co.country, \n",
    "        a.postal_code,          \n",
    "        a.phone,                 \n",
    "        c.last_update as src_last_update\n",
    "    FROM sakila.customer c\n",
    "    JOIN sakila.address a ON c.address_id = a.address_id\n",
    "    JOIN sakila.city ci ON a.city_id = ci.city_id\n",
    "    JOIN sakila.country co ON ci.country_id = co.country_id\n",
    "    WHERE c.last_update > :watermark;\n",
    "\"\"\")\n",
    "\n",
    "# test_select_query(dim_customer_extract_sql, src_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd97550-a694-43b3-b748-2b7b3cb579b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSERT DATA STATEMENTS ===== #\n",
    "\n",
    "\n",
    "# Define the Load SQL (Upsert)\n",
    "dim_store_load_sql = text(\"\"\"\n",
    "    INSERT INTO dim_store (\n",
    "        store_id, \n",
    "        address, \n",
    "        address2, \n",
    "        district, \n",
    "        city, \n",
    "        country, \n",
    "        postal_code, \n",
    "        phone, \n",
    "        src_last_update\n",
    "    ) VALUES (\n",
    "        :store_id, \n",
    "        :address, \n",
    "        :address2, \n",
    "        :district, \n",
    "        :city, \n",
    "        :country, \n",
    "        :postal_code, \n",
    "        :phone, \n",
    "        :src_last_update\n",
    "    )\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        address         = VALUES(address),\n",
    "        address2        = VALUES(address2),      -- Added\n",
    "        district        = VALUES(district),      -- Added\n",
    "        city            = VALUES(city),\n",
    "        country         = VALUES(country),       -- Added\n",
    "        postal_code     = VALUES(postal_code),   -- Added\n",
    "        phone           = VALUES(phone),\n",
    "        src_last_update = VALUES(src_last_update);\n",
    "\"\"\")\n",
    "\n",
    "dim_customer_load_sql = text(\"\"\"\n",
    "    INSERT INTO dim_customer (\n",
    "        customer_id, \n",
    "        store_id, \n",
    "        first_name, \n",
    "        last_name, \n",
    "        email, \n",
    "        activebool, \n",
    "        active, \n",
    "        create_date, \n",
    "        address, \n",
    "        address2, \n",
    "        district, \n",
    "        city, \n",
    "        country, \n",
    "        postal_code, \n",
    "        phone, \n",
    "        src_last_update\n",
    "    ) VALUES (\n",
    "        :customer_id, \n",
    "        :store_id, \n",
    "        :first_name, \n",
    "        :last_name, \n",
    "        :email, \n",
    "        :activebool, \n",
    "        :active, \n",
    "        :create_date, \n",
    "        :address, \n",
    "        :address2, \n",
    "        :district, \n",
    "        :city, \n",
    "        :country, \n",
    "        :postal_code, \n",
    "        :phone, \n",
    "        :src_last_update\n",
    "    )\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        store_id        = VALUES(store_id),\n",
    "        first_name      = VALUES(first_name),\n",
    "        last_name       = VALUES(last_name),\n",
    "        email           = VALUES(email),\n",
    "        activebool      = VALUES(activebool),\n",
    "        active          = VALUES(active),\n",
    "        address         = VALUES(address),\n",
    "        address2        = VALUES(address2),\n",
    "        district        = VALUES(district),\n",
    "        city            = VALUES(city),\n",
    "        country         = VALUES(country),\n",
    "        postal_code     = VALUES(postal_code),\n",
    "        phone           = VALUES(phone),\n",
    "        src_last_update = VALUES(src_last_update);\n",
    "\"\"\")\n",
    "\n",
    "# test_select_query(\"DESCRIBE dim_customer\", tgt_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f960db9-ee12-45ca-b72f-6f62691694d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD ABOVE DATA TO UPSERT LIST\n",
    "\n",
    "upsert_list = [\n",
    "    {\n",
    "        \"table_name\": \"dim_store\",\n",
    "        \"extract_sql\": dim_store_extract_sql,\n",
    "        \"load_sql\": dim_store_load_sql,\n",
    "    },\n",
    "    {\n",
    "        \"table_name\": \"dim_customer\",\n",
    "        \"extract_sql\": dim_customer_extract_sql,\n",
    "        \"load_sql\": dim_customer_load_sql,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b806bd-e9da-418b-bfec-579a81d23c5b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Run Incremental Load for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008510d-4d72-4ffc-8156-8e707680f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PERFORM INCREMENTAL LOAD ===== #\n",
    "\n",
    "# _initialise_etl_state()\n",
    "# _clear_table_data([\"dim_customer\", \"dim_store\"], tgt_engine, force=False)\n",
    "\n",
    "\n",
    "        \n",
    "upsert_data(upsert_list, src_engine, tgt_engine)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
